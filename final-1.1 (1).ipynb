{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca914a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb5da93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a4c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b1d062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.19531250e-01, -3.36914062e-02, -2.07519531e-02,  5.24902344e-02,\n",
       "        5.93261719e-02, -1.28906250e-01,  1.28784180e-02, -8.44726562e-02,\n",
       "        2.27539062e-01, -1.21093750e-01, -1.64062500e-01,  6.44531250e-02,\n",
       "        4.10156250e-02, -8.54492188e-03, -8.83789062e-02,  8.69140625e-02,\n",
       "        5.68847656e-02, -1.12304688e-01, -2.03125000e-01, -2.15820312e-01,\n",
       "        2.94921875e-01, -2.66113281e-02, -8.10546875e-02,  1.19628906e-02,\n",
       "       -9.70458984e-03,  3.22265625e-02,  5.93261719e-02,  1.26953125e-01,\n",
       "        1.24023438e-01,  1.15966797e-02, -1.03027344e-01, -1.53320312e-01,\n",
       "       -2.22656250e-01, -7.61718750e-02, -1.62353516e-02, -7.22656250e-02,\n",
       "        4.02832031e-02, -2.24609375e-01,  2.27539062e-01, -8.39843750e-02,\n",
       "       -3.02734375e-02,  2.75878906e-02, -1.25000000e-01,  1.27929688e-01,\n",
       "       -9.27734375e-02, -4.85839844e-02,  6.93359375e-02, -1.35742188e-01,\n",
       "       -1.88476562e-01,  5.39550781e-02,  2.46093750e-01,  7.93457031e-03,\n",
       "        1.53320312e-01,  8.05664062e-02, -1.65039062e-01,  7.76367188e-02,\n",
       "       -1.26342773e-02, -1.61132812e-01, -3.36914062e-02,  1.61132812e-01,\n",
       "       -3.65234375e-01,  1.30859375e-01, -1.95312500e-01,  8.36181641e-03,\n",
       "       -4.63867188e-03,  1.14746094e-01, -2.05078125e-02,  2.59765625e-01,\n",
       "        4.39453125e-02, -3.24218750e-01,  1.10839844e-01, -2.34375000e-01,\n",
       "        2.99072266e-02, -5.15136719e-02, -1.03027344e-01, -7.22656250e-02,\n",
       "        1.88476562e-01,  5.22460938e-02,  1.27563477e-02,  3.76953125e-01,\n",
       "        1.80664062e-01, -1.91406250e-01, -6.64062500e-02,  1.66992188e-01,\n",
       "        1.24511719e-01, -2.47802734e-02, -1.11328125e-01,  2.36328125e-01,\n",
       "       -2.42919922e-02, -1.54296875e-01,  1.15234375e-01, -7.27539062e-02,\n",
       "        1.54296875e-01,  2.24609375e-01, -7.66601562e-02, -9.57031250e-02,\n",
       "       -8.78906250e-02, -1.73339844e-02,  9.76562500e-02, -1.03027344e-01,\n",
       "        6.59179688e-02,  2.00195312e-02,  1.73828125e-01,  3.90625000e-02,\n",
       "       -7.27539062e-02, -1.27929688e-01, -1.67968750e-01,  3.79562378e-04,\n",
       "       -7.47070312e-02, -2.00195312e-01,  1.55273438e-01,  2.24609375e-01,\n",
       "       -1.70898438e-01, -1.98974609e-02,  2.85156250e-01, -1.04492188e-01,\n",
       "       -1.55273438e-01,  2.03125000e-01,  4.10156250e-01,  6.44531250e-02,\n",
       "       -8.54492188e-02, -1.06811523e-02, -1.70898438e-01,  1.78710938e-01,\n",
       "        6.10351562e-02, -8.54492188e-02, -2.77099609e-02, -3.90625000e-02,\n",
       "       -1.19628906e-01,  8.85009766e-03, -1.61132812e-01, -2.05078125e-01,\n",
       "       -3.00781250e-01, -1.15234375e-01, -6.34765625e-02,  1.11816406e-01,\n",
       "        7.81250000e-02,  1.36718750e-01,  2.67333984e-02, -2.97851562e-02,\n",
       "        2.57812500e-01,  5.00488281e-03, -2.01171875e-01,  1.77734375e-01,\n",
       "        2.39257812e-01, -5.24902344e-02,  3.75976562e-02, -8.54492188e-02,\n",
       "        1.72119141e-02,  1.82617188e-01,  9.42382812e-02, -1.96289062e-01,\n",
       "        6.54296875e-02,  1.64062500e-01,  1.67968750e-01, -3.16406250e-01,\n",
       "        2.03125000e-01, -3.61328125e-02, -6.12792969e-02,  9.22851562e-02,\n",
       "       -6.39648438e-02,  8.85009766e-03,  1.02050781e-01,  9.76562500e-02,\n",
       "       -2.96630859e-02, -5.07812500e-02,  3.33984375e-01, -7.86132812e-02,\n",
       "        1.97265625e-01, -5.34667969e-02,  2.24609375e-01, -3.80859375e-02,\n",
       "       -5.83496094e-02,  1.95312500e-01,  3.90625000e-02, -2.27539062e-01,\n",
       "       -1.06201172e-02, -6.29882812e-02, -1.14257812e-01,  1.10626221e-03,\n",
       "       -2.92968750e-01,  3.51562500e-02,  7.61718750e-02, -1.00097656e-01,\n",
       "       -1.16699219e-01,  1.05957031e-01, -9.17968750e-02,  1.70898438e-02,\n",
       "        9.32617188e-02,  4.80957031e-02, -2.89306641e-02, -2.15820312e-01,\n",
       "       -2.73437500e-02, -4.85839844e-02,  8.78906250e-02, -9.42382812e-02,\n",
       "        1.36718750e-01, -2.44140625e-01, -7.47680664e-03,  2.99072266e-03,\n",
       "        7.86132812e-02,  2.73437500e-01,  2.08740234e-02, -2.57568359e-02,\n",
       "        6.93359375e-02,  3.11279297e-02, -1.30859375e-01,  6.78710938e-02,\n",
       "        2.46093750e-01,  1.92382812e-01,  2.92968750e-01,  9.22851562e-02,\n",
       "        6.44531250e-02, -2.72216797e-02, -4.33593750e-01, -4.71191406e-02,\n",
       "        1.34765625e-01,  2.50000000e-01,  1.77734375e-01, -1.81640625e-01,\n",
       "        1.09252930e-02,  1.58203125e-01, -8.54492188e-02,  5.95092773e-03,\n",
       "        1.63085938e-01, -2.05078125e-01, -1.34887695e-02,  4.29687500e-02,\n",
       "        7.93457031e-03, -7.56835938e-02,  4.71191406e-02, -2.00195312e-01,\n",
       "       -3.51562500e-02, -6.98242188e-02,  1.84570312e-01,  8.60595703e-03,\n",
       "        1.50390625e-01,  2.89306641e-02,  2.15820312e-01,  5.05371094e-02,\n",
       "       -3.28125000e-01, -1.40380859e-02, -1.47460938e-01, -9.13085938e-02,\n",
       "       -1.00097656e-01, -8.30078125e-02, -1.48437500e-01, -9.91210938e-02,\n",
       "        1.21459961e-02, -2.38037109e-02, -1.44531250e-01,  1.62109375e-01,\n",
       "       -6.68945312e-02, -1.55029297e-02, -4.76074219e-02,  1.14257812e-01,\n",
       "       -7.12890625e-02,  9.32617188e-02,  1.33789062e-01,  3.66210938e-02,\n",
       "        1.09863281e-01,  1.66992188e-01, -1.50390625e-01, -1.54296875e-01,\n",
       "       -1.72119141e-02, -9.03320312e-02, -1.29882812e-01,  5.07812500e-02,\n",
       "       -2.53906250e-01, -8.83789062e-02,  3.86718750e-01,  2.08007812e-01,\n",
       "        1.78710938e-01,  3.73535156e-02,  6.05468750e-02, -4.37011719e-02,\n",
       "       -8.98437500e-02, -3.02734375e-01, -1.52343750e-01,  1.19018555e-02,\n",
       "        1.30859375e-01, -4.07714844e-02, -6.31713867e-03,  2.42187500e-01,\n",
       "        3.56445312e-02, -7.27539062e-02, -1.29394531e-02,  2.11914062e-01,\n",
       "        1.56250000e-01, -1.67968750e-01,  1.06445312e-01, -7.12890625e-02,\n",
       "        1.80664062e-01,  2.87109375e-01, -2.24609375e-02, -8.98437500e-02,\n",
       "        9.64355469e-03, -2.37304688e-01, -1.98242188e-01, -1.06933594e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0045f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking average of word_embeddings of each word of review. So finally we would have a 300-dim vector of each sentence to feed in our ML model\n",
    "def sent_vec(sent):\n",
    "    vector_size = wv.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    # print(wv_res)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in wv:\n",
    "            ctr += 1\n",
    "            wv_res += wv[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be496ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_tidy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great love it</td>\n",
       "      <td>Great love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Lots of ads&lt;br /&gt;Slow processing speed&lt;br /&gt;Oc...</td>\n",
       "      <td>Lots Slow processing speed Occasionally shuts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent unit.  The versatility of this table...</td>\n",
       "      <td>Excellent unit versatility tablet besides comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I bought this on Amazon Prime so I ended up bu...</td>\n",
       "      <td>bought Amazon Prime ended buying camera okay l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>All Amazon products continue to meet my expect...</td>\n",
       "      <td>Amazon products continue meet expectations</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                        review_body  \\\n",
       "0          1                                      Great love it   \n",
       "1          0  Lots of ads<br />Slow processing speed<br />Oc...   \n",
       "2          1  Excellent unit.  The versatility of this table...   \n",
       "3          1  I bought this on Amazon Prime so I ended up bu...   \n",
       "4          1  All Amazon products continue to meet my expect...   \n",
       "\n",
       "                                         review_tidy  \n",
       "0                                         Great love  \n",
       "1  Lots Slow processing speed Occasionally shuts ...  \n",
       "2  Excellent unit versatility tablet besides comp...  \n",
       "3  bought Amazon Prime ended buying camera okay l...  \n",
       "4         Amazon products continue meet expectations  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\karan\\Downloads\\review_data.csv\")\n",
    "df = df.iloc[:, [1, 2, 3]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270cf731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_tidy'] = df['review_tidy'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "496e0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "501db632",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "437885c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing and lemmitizing\n",
    "def tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "\n",
    "\n",
    "    # print(doc)\n",
    "    # print(type(doc))\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc]\n",
    "\n",
    "   \n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67cbec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_tidy</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great love it</td>\n",
       "      <td>Great love</td>\n",
       "      <td>[great, love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Lots of ads&lt;br /&gt;Slow processing speed&lt;br /&gt;Oc...</td>\n",
       "      <td>Lots Slow processing speed Occasionally shuts ...</td>\n",
       "      <td>[lot, slow, processing, speed, occasionally, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent unit.  The versatility of this table...</td>\n",
       "      <td>Excellent unit versatility tablet besides comp...</td>\n",
       "      <td>[excellent, unit, versatility, tablet, besides...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I bought this on Amazon Prime so I ended up bu...</td>\n",
       "      <td>bought Amazon Prime ended buying camera okay l...</td>\n",
       "      <td>[buy, amazon, prime, end, buy, camera, okay, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>All Amazon products continue to meet my expect...</td>\n",
       "      <td>Amazon products continue meet expectations</td>\n",
       "      <td>[amazon, product, continue, meet, expectation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                        review_body  \\\n",
       "0          1                                      Great love it   \n",
       "1          0  Lots of ads<br />Slow processing speed<br />Oc...   \n",
       "2          1  Excellent unit.  The versatility of this table...   \n",
       "3          1  I bought this on Amazon Prime so I ended up bu...   \n",
       "4          1  All Amazon products continue to meet my expect...   \n",
       "\n",
       "                                         review_tidy  \\\n",
       "0                                         Great love   \n",
       "1  Lots Slow processing speed Occasionally shuts ...   \n",
       "2  Excellent unit versatility tablet besides comp...   \n",
       "3  bought Amazon Prime ended buying camera okay l...   \n",
       "4         Amazon products continue meet expectations   \n",
       "\n",
       "                                              tokens  \n",
       "0                                      [great, love]  \n",
       "1  [lot, slow, processing, speed, occasionally, s...  \n",
       "2  [excellent, unit, versatility, tablet, besides...  \n",
       "3  [buy, amazon, prime, end, buy, camera, okay, l...  \n",
       "4     [amazon, product, continue, meet, expectation]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = df['review_tidy'].apply(tokenizer)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "088b45a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_tidy</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great love it</td>\n",
       "      <td>Great love</td>\n",
       "      <td>[great, love]</td>\n",
       "      <td>[0.058268229166666664, 0.0185546875, -0.000854...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Lots of ads&lt;br /&gt;Slow processing speed&lt;br /&gt;Oc...</td>\n",
       "      <td>Lots Slow processing speed Occasionally shuts ...</td>\n",
       "      <td>[lot, slow, processing, speed, occasionally, s...</td>\n",
       "      <td>[0.0572662353515625, -0.023372395833333334, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent unit.  The versatility of this table...</td>\n",
       "      <td>Excellent unit versatility tablet besides comp...</td>\n",
       "      <td>[excellent, unit, versatility, tablet, besides...</td>\n",
       "      <td>[0.04113743222992996, 0.013691112912934402, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I bought this on Amazon Prime so I ended up bu...</td>\n",
       "      <td>bought Amazon Prime ended buying camera okay l...</td>\n",
       "      <td>[buy, amazon, prime, end, buy, camera, okay, l...</td>\n",
       "      <td>[0.052636564447638694, -0.014119008953651686, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>All Amazon products continue to meet my expect...</td>\n",
       "      <td>Amazon products continue meet expectations</td>\n",
       "      <td>[amazon, product, continue, meet, expectation]</td>\n",
       "      <td>[-0.10062662760416667, 0.09965006510416667, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                        review_body  \\\n",
       "0          1                                      Great love it   \n",
       "1          0  Lots of ads<br />Slow processing speed<br />Oc...   \n",
       "2          1  Excellent unit.  The versatility of this table...   \n",
       "3          1  I bought this on Amazon Prime so I ended up bu...   \n",
       "4          1  All Amazon products continue to meet my expect...   \n",
       "\n",
       "                                         review_tidy  \\\n",
       "0                                         Great love   \n",
       "1  Lots Slow processing speed Occasionally shuts ...   \n",
       "2  Excellent unit versatility tablet besides comp...   \n",
       "3  bought Amazon Prime ended buying camera okay l...   \n",
       "4         Amazon products continue meet expectations   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                      [great, love]   \n",
       "1  [lot, slow, processing, speed, occasionally, s...   \n",
       "2  [excellent, unit, versatility, tablet, besides...   \n",
       "3  [buy, amazon, prime, end, buy, camera, okay, l...   \n",
       "4     [amazon, product, continue, meet, expectation]   \n",
       "\n",
       "                                                 vec  \n",
       "0  [0.058268229166666664, 0.0185546875, -0.000854...  \n",
       "1  [0.0572662353515625, -0.023372395833333334, -0...  \n",
       "2  [0.04113743222992996, 0.013691112912934402, -0...  \n",
       "3  [0.052636564447638694, -0.014119008953651686, ...  \n",
       "4  [-0.10062662760416667, 0.09965006510416667, -0...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vec'] = df['tokens'].apply(sent_vec)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6be5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['vec'].to_list()\n",
    "Y = df['sentiment'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bccb371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size = .3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c8235eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from xgboost import plot_tree, plot_importance\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bf8c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, title = \"\"):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_valid)\n",
    "    print(\"F1-score of {} is {}\".format(title, f1_score(y_valid, y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6766e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "599fff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of LogisticRegression is 0.9374960659658841\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=42)\n",
    "evaluation(log_reg, \"LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c82f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm(trial):\n",
    "    kernel = trial.suggest_categorical('kernel', ['poly', 'rbf'])\n",
    "    C = trial.suggest_float('C', 0.01, 100, log = True)\n",
    "    gamma = trial.suggest_float('gamma', 0.01, 100, log = True)\n",
    "    model = SVC(kernel = kernel, C = C, gamma = gamma, random_state = 42)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_valid)\n",
    "    return f1_score(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851abe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed = 1)\n",
    "study = optuna.create_study(direction = 'maximize', sampler = sampler)\n",
    "study.optimize(objective_svm, n_trials = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xg(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 100, 1000)\n",
    "    learning_rate = trail.suggest_float('learning_rate', 0.001, 0.1, log = True)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    \n",
    "    model = XGBClassifier(n_estimators = n_estimators, learning_rate = learning_rate, max_depth = max_depth, random_state = 42)\n",
    "    model.fit(x_train_tf, y_train_tf)\n",
    "    y_pred_tf = model.predict(x_valid_tf)\n",
    "    return f1_score(y_valid_tf, y_pred_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc01f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed = 1)\n",
    "study = optuna.create_study(direction = 'maximize', sampler = sampler)\n",
    "study.optimize(objective_xg, n_trials = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37f87c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
